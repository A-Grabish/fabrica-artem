Py Style Transfer uses a abbriviated version of a VGG19 pretrained framework (with the addition of Markov Random Field at higher levels), pytorch, and Neural Doodle's semantic maps to transfer the style of one image onto the content of another.  The content loss function uses a lower layer than other methods to preserve more content detail.  Style loss is caculated using only three of the middle layers, versus the four or five layer calculation which use low, middle and high layers favored by other programers.
The synthesized images are visually coherent, and preserve the content of the orginal well while implementing elements of the style image to suppliment the coloring and shading.  The use of only the middle layers for style transfer eliminates the occurance of phantom images where large portions of the style image can be seen faded, but otherwise unatlered in the final synthesized image.  However, this also limits the about of style image detail that can be seen in the final image, resulting in more of an approximation of style rather than a direct transference of style.